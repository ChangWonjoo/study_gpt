{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "# set_llm_cache(SQLiteCache(\"llm_cache.db\"))\n",
    "# set_debug(True)\n",
    "\n",
    "# Create a chat model instance\n",
    "# chat = ChatOpenAI(\n",
    "#    temperature=0.1,\n",
    "# #    streaming=True,\n",
    "# #    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "# )\n",
    "\n",
    "# chat.predict(\"이탈리안 파스타는 어떻게 만들어?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat.predict(\"이탈리안 파스타는 어떻게 만들어?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.callbacks import get_openai_callback\n",
    "\n",
    "# with get_openai_callback() as usage:\n",
    "#     a = chat.predict(\"이탈리안 피자의 레시피는?\")\n",
    "#     b = chat.predict(\"이탈리안 파스타의 레시피는?\")\n",
    "#     print(a,b,\"\\n\")\n",
    "#     print(usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wonjooLAPTOP\\Dropbox\\github\\study_gpt\\env\\Lib\\site-packages\\langchain\\llms\\openai.py:216: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "c:\\Users\\wonjooLAPTOP\\Dropbox\\github\\study_gpt\\env\\Lib\\site-packages\\langchain\\llms\\openai.py:811: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OpenAIChat(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_kwargs={'temperature': 0.1, 'max_tokens': 50, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'request_timeout': None, 'logit_bias': {}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.llms.loading import load_llm\n",
    "\n",
    "# chat = OpenAI(temperature=0.1, max_tokens=50, model=\"gpt-3.5-turbo\")\n",
    "# chat.save(\"models.json\")\n",
    "\n",
    "chat = load_llm(\"models.json\")\n",
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
